% Encoding: UTF-8
% Try to keep this list in alphabetical order based on citing name

@article{Colombo2012,
  author    = {Diego Colombo and Marloes H. Maathuis and Markus Kalisch and Thomas S. Richardson},
  title     = {{Learning high-dimensional directed acyclic graphs with latent and selection variables}},
  volume    = {40},
  journal   = {The Annals of Statistics},
  number    = {1},
  publisher = {Institute of Mathematical Statistics},
  pages     = {294 -- 321},
  keywords  = {Causal structure learning, consistency, FCI algorithm, high-dimensionality, maximal ancestral graphs (MAGs), partial ancestral graphs (PAGs), RFCI algorithm, Sparsity},
  year      = {2012},
  doi       = {10.1214/11-AOS940},
  url       = {https://doi.org/10.1214/11-AOS940}
}

@article{Colombo2012_MPC,
  author  = {Colombo, Diego and Maathuis, Marloes},
  year    = {2012},
  month   = {11},
  pages   = {},
  title   = {Order-Independent Constraint-Based Causal Structure Learning},
  volume  = {15},
  journal = {Journal of Machine Learning Research}
}

@article{Gamez2011,
  author  = {Gámez, José and Mateo, Juan and Puerta, Jose},
  year    = {2011},
  month   = {05},
  pages   = {106-148},
  title   = {Learning Bayesian networks by hill climbing: Efficient methods based on progressive restriction of the neighborhood},
  volume  = {22},
  journal = {Data Mining and Knowledge Discovery},
  doi     = {10.1007/s10618-010-0178-6}
}

@article{Meek1995,
  author  = {Meek, Christopher},
  year    = {2013},
  month   = {02},
  pages   = {},
  title   = {Causal Inference and Causal Explanation with Background Knowledge},
  volume  = {2},
  journal = {Proceedings of Eleventh Conference on Uncertainty in Artificial Intelligence, Montreal, QU}
}

@book{Neapolitan2003,
  author    = {Neapolitan, Richard},
  year      = {2003},
  month     = {01},
  pages     = {},
  title     = {Learning Bayesian Networks},
  isbn      = {9780123704771},
  publisher = {Pearson},
  doi       = {10.1145/1327942.1327961}
}

@book{Pearl_causality_2009,
  author    = {Pearl, Judea},
  title     = {Causality: Models, Reasoning and Inference},
  year      = {2009},
  isbn      = {052189560X},
  publisher = {Cambridge University Press},
  address   = {USA},
  edition   = {2nd},
  abstract  = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.}
}

@article{pearl2014confounding,
  title     = {Confounding equivalence in causal inference},
  author    = {Pearl, Judea and Paz, Azaria},
  journal   = {Journal of Causal Inference},
  volume    = {2},
  number    = {1},
  pages     = {75--93},
  year      = {2014},
  publisher = {De Gruyter}
}

@article{ramsey2012adjacency,
  title   = {Adjacency-faithfulness and conservative causal inference},
  author  = {Ramsey, Joseph and Zhang, Jiji and Spirtes, Peter L},
  journal = {arXiv preprint arXiv:1206.6843},
  year    = {2012}
}

@article{Runge_pcmci_2019,
  author   = {Jakob Runge  and Peer Nowack  and Marlene Kretschmer  and Seth Flaxman  and Dino Sejdinovic },
  title    = {Detecting and quantifying causal associations in large nonlinear time series datasets},
  journal  = {Science Advances},
  volume   = {5},
  number   = {11},
  pages    = {eaau4996},
  year     = {2019},
  doi      = {10.1126/sciadv.aau4996},
  url      = {https://www.science.org/doi/abs/10.1126/sciadv.aau4996},
  eprint   = {https://www.science.org/doi/pdf/10.1126/sciadv.aau4996},
  abstract = {A novel causal discovery method for estimating nonlinear interdependency networks from large time series datasets. Identifying causal relationships and quantifying their strength from observational time series data are key problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body. Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets. We validate the method on time series of well-understood physical mechanisms in the climate system and the human heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely new possibilities to discover and quantify causal networks from time series across a range of research fields.}
}

@inproceedings{Tian1998FindingMD,
  title  = {Finding Minimal D-separators},
  author = {Jing-jing Tian and Azaria Paz},
  year   = {1998}
}

@book{Spirtes1993,
  author    = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
  year      = {1993},
  month     = {01},
  pages     = {},
  title     = {Causation, Prediction, and Search},
  volume    = {81},
  isbn      = {978-1-4612-7650-0},
  doi       = {10.1007/978-1-4612-2748-9},
  publisher = {The MIT Press}
}


@inproceedings{van-der-zander20a,
  title     = {Finding Minimal d-separators in Linear Time and Applications},
  author    = {van der Zander, Benito and Li\'{s}kiewicz, Maciej},
  booktitle = {Proceedings of The 35th Uncertainty in Artificial Intelligence Conference},
  pages     = {637--647},
  year      = {2020},
  editor    = {Adams, Ryan P. and Gogate, Vibhav},
  volume    = {115},
  series    = {Proceedings of Machine Learning Research},
  month     = {22--25 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v115/van-der-zander20a/van-der-zander20a.pdf},
  url       = {https://proceedings.mlr.press/v115/van-der-zander20a.html},
  abstract  = {The study of graphical causal models is fundamentally the study of separations and conditional independences. We provide linear time algorithms for two graphical primitives: to test, if a given set is a minimal d-separator, and to find a minimal d-separator in directed acyclic graphs (DAGs), completed partially directed acyclic graphs (CPDAGs) and restricted chain graphs (RCGs) as well as minimal m-separators in ancestral graphs (AGs). These algorithms improve the runtime of the best previously known algorithms for minimal separators  that are based on moralization and thus require quadratic time to construct and handle the moral graph. (Minimal) separating sets have important applications like finding (minimal) covariate adjustment sets or conditional instrumental variables.}
}

@article{Zhang2008,
  title    = {On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias},
  journal  = {Artificial Intelligence},
  volume   = {172},
  number   = {16},
  pages    = {1873-1896},
  year     = {2008},
  issn     = {0004-3702},
  doi      = {https://doi.org/10.1016/j.artint.2008.08.001},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370208001008},
  author   = {Jiji Zhang},
  keywords = {Ancestral graphs, Automated causal discovery, Bayesian networks, Causal models, Markov equivalence, Latent variables},
  abstract = {Causal discovery becomes especially challenging when the possibility of latent confounding and/or selection bias is not assumed away. For this task, ancestral graph models are particularly useful in that they can represent the presence of latent confounding and selection effect, without explicitly invoking unobserved variables. Based on the machinery of ancestral graphs, there is a provably sound causal discovery algorithm, known as the FCI algorithm, that allows the possibility of latent confounders and selection bias. However, the orientation rules used in the algorithm are not complete. In this paper, we provide additional orientation rules, augmented by which the FCI algorithm is shown to be complete, in the sense that it can, under standard assumptions, discover all aspects of the causal structure that are uniquely determined by facts of probabilistic dependence and independence. The result is useful for developing any causal discovery and reasoning system based on ancestral graph models.}
}

@inproceedings{Zhang2011,
  author    = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Sch\"{o}lkopf, Bernhard},
  title     = {Kernel-Based Conditional Independence Test and Application in Causal Discovery},
  year      = {2011},
  isbn      = {9780974903972},
  publisher = {AUAI Press},
  address   = {Arlington, Virginia, USA},
  abstract  = {Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.},
  booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages     = {804–813},
  numpages  = {10},
  location  = {Barcelona, Spain},
  series    = {UAI'11}
}
